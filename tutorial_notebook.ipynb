{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trainer1/dvrprl/dvrprl/ppo.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dvrprl.networks import ParallelMultilayerPerceptron, Transformer, RecurrentNeuralNetwork\n",
    "from dvrprl.ppo import PPOAgent\n",
    "from dvrprl.tsp_env import TSPEnv\n",
    "from dvrprl.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to setup the environment, to do this we need to import the environment class and instantiate it. The environment class is called TSPEnv and is located in dvrprl/tsp_env.py. The TSPEnv class inherits from the base class Env from the gym library. The TSPEnv class has the following arguments: \n",
    "# num_nodes: Number of nodes in each generated graph. Defaults to 20.\n",
    "# seed: Seed of the environment. Defaults to 123.\n",
    "\n",
    "# First, we need to import the environment\n",
    "from dvrprl.tsp_env import TSPEnv\n",
    "\n",
    "# Instantiate the environment\n",
    "env = TSPEnv()\n",
    "\n",
    "# Instantiate the environment with a different number of nodes\n",
    "env = TSPEnv(num_nodes=10)\n",
    "\n",
    "# Instantiate the environment with a set random seed\n",
    "env = TSPEnv(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to define an agent, the agent contains all of the code needed to train a reinforcement learning agent in an environment. The algorithm used to train the agent differs for the DQN and PPO agents. The DQN agent uses the DQN algorithm. The PPO agent uses the PPO algorithm (More on this at a later point). The agent is defined in dvrprl/agent.py:\n",
    "\n",
    "# When defining an agent, it is necessary to supply it with one or more neural networks to act as trainable function approximators\n",
    "# For a DQN agent this is a Q network which is copied within the agent to make the target network.\n",
    "\n",
    "# To define a Parallel Multilayer Perceptron network\n",
    "from dvrprl.networks import ParallelMultilayerPerceptron\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "# Note that the mode argument is set to \"action-value\" to indicate that the network should output action-values. This will be the only mode necessary for the DQN agent. Other modes will be necessary for the PPO agent. For the dvrprl environment the input features will always be 2, the x and y coordinates of the node. The hidden layers argument is a list of hidden layer sizes. The first hidden layer will have 64 nodes, the second hidden layer will have 64 nodes. The final layer will have 1 node, the action-value of the state-action pair. The network can be defined as follows:\n",
    "q_network = ParallelMultilayerPerceptron(in_features=4, embedding_dim=EMBEDDING_DIM, hidden_layers=[64, 64], mode=\"action-value\")\n",
    "\n",
    "# The only argument that needs to be passed when creating a dqn agent is the value_network but there are many other arguments that can be passed to the agent. The full list of arguments can be found in the documentation. For now, the agent can be created as follows:\n",
    "agent = DQNAgent(value_network=q_network)\n",
    "\n",
    "# Give a name to the folder in which the logs will be saved\n",
    "\n",
    "logdir = \"logs/dqn-test-pmlp-24-07-23\"\n",
    "\n",
    "# The agent can now be trained in the environment. The agent can be trained for 1000 episodes with the following code:\n",
    "# Again, there are more arguments that can be passed to the train method. The full list of arguments can be found in the documentation. For now, the agent can be trained as follows:\n",
    "\n",
    "agent.train(env, episodes=1000, batch_size=64, logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Give name of folder to save the results\n",
    "\n",
    "logdir = \"logs/dqn-pmlp-24-07-23\"\n",
    "\n",
    "agent.train(env, episodes=100, batch_size=64, logdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a9566508484aa387e0a4e33214bba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdc09911213407da5d62674cad78909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?episodes/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Give name of folder to save the results, change this each time of results will be overwritten\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logdir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogs/ppo-pmlp-24-07-23\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(env, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, logdir\u001b[39m=\u001b[39;49mlogdir)\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/ppo.py:233\u001b[0m, in \u001b[0;36mAgent.train\u001b[0;34m(self, env, episodes, epochs, verbose, save_freq, logdir, batch_size)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs), unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mclear()\n\u001b[0;32m--> 233\u001b[0m     return_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_episodes(env, episodes, buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer)\n\u001b[1;32m    234\u001b[0m     dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mget(batch_size\u001b[39m=\u001b[39mbatch_size, normalize_advantages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_advantages)\n\u001b[1;32m    235\u001b[0m     policy_history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_policy_model(dataloader, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_updates)\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/ppo.py:333\u001b[0m, in \u001b[0;36mAgent.run_episodes\u001b[0;34m(self, env, episodes, buffer)\u001b[0m\n\u001b[1;32m    328\u001b[0m history \u001b[39m=\u001b[39m {\n\u001b[1;32m    329\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mzeros(episodes),\n\u001b[1;32m    330\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mzeros(episodes)\n\u001b[1;32m    331\u001b[0m }\n\u001b[1;32m    332\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(episodes), unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepisodes\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 333\u001b[0m     R, L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_episode(env, buffer\u001b[39m=\u001b[39;49mbuffer)\n\u001b[1;32m    334\u001b[0m     history[\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m R\n\u001b[1;32m    335\u001b[0m     history[\u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m][i] \u001b[39m=\u001b[39m L\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/ppo.py:297\u001b[0m, in \u001b[0;36mAgent.run_episode\u001b[0;34m(self, env, buffer, render)\u001b[0m\n\u001b[1;32m    295\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m--> 297\u001b[0m     action, logprob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_action(obs, return_logprob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_network \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/ppo.py:193\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[0;34m(self, obs, return_logprob)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action\u001b[39m(\u001b[39mself\u001b[39m, obs: np\u001b[39m.\u001b[39mndarray, return_logprob: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    181\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    Get an action from the policy given an observation.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m        float: Value of the state.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     logpi \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_network(torch\u001b[39m.\u001b[39;49mfrom_numpy(obs)\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)), (\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m    194\u001b[0m     action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mCategorical(logits\u001b[39m=\u001b[39mlogpi)\u001b[39m.\u001b[39msample()\n\u001b[1;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m return_logprob:\n",
      "File \u001b[0;32m~/dvrprl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/networks.py:56\u001b[0m, in \u001b[0;36mParallelMultilayerPerceptron.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     43\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    forward pass of the network.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_layer(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_stack:\n\u001b[1;32m     58\u001b[0m         embedding \u001b[39m=\u001b[39m layer(embedding)\n",
      "File \u001b[0;32m~/dvrprl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dvrprl/dvrprl/layers.py:35\u001b[0m, in \u001b[0;36mParallelEmbeddingLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layers:\n\u001b[1;32m     34\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(layer(\u001b[39minput\u001b[39m))\n\u001b[0;32m---> 35\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/dvrprl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dvrprl/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing PPO agent\n",
    "\n",
    "env = TSPEnv(num_nodes=20)\n",
    "\n",
    "# Define the networks\n",
    "policy_network = ParallelMultilayerPerceptron(in_features=4, embedding_dim=128, hidden_layers=[128, 128], mode=\"policy\")\n",
    "\n",
    "value_network = ParallelMultilayerPerceptron(in_features=4, embedding_dim=128, hidden_layers=[128, 128], mode=\"value\")\n",
    "\n",
    "# Define the agent\n",
    "\n",
    "agent = PPOAgent(policy_network=policy_network, value_network=value_network)\n",
    "\n",
    "# Give name of folder to save the results, change this each time of results will be overwritten\n",
    "\n",
    "logdir = \"logs/ppo-pmlp-24-07-23\"\n",
    "\n",
    "agent.train(env, epochs=10, episodes=100, batch_size=64, logdir=logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
